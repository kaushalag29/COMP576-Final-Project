{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "metadata": {
        "id": "psGOt50xmfvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from annoy import AnnoyIndex\n",
        "import cv2\n",
        "from collections import defaultdict\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer"
      ],
      "metadata": {
        "id": "NsyWQn2ClFHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i762bKZrjtja"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "REPORTS_PATH = '/content/drive/MyDrive/Colab Notebooks/Data/IU X-Ray New/ecgen-radiology'\n",
        "IMAGES_DIR = '/content/drive/My Drive/Colab Notebooks/Data/IUXRay/Scanned Images Unzipped/Scanned Images/'\n",
        "\n",
        "ids = []\n",
        "impressions = []\n",
        "findings = []\n",
        "images = []\n",
        "\n",
        "for filename in  sorted(os.listdir(REPORTS_PATH)):\n",
        "    root = ET.parse(os.path.join(REPORTS_PATH,filename)).getroot()\n",
        "    imageId = []\n",
        "\n",
        "    for child in root.iter():\n",
        "        if child.tag == 'pmcId':\n",
        "            ids.append(child.attrib['id'])\n",
        "        if child.tag == 'AbstractText':\n",
        "            if child.attrib['Label'] == 'IMPRESSION':\n",
        "                impressions.append(child.text)\n",
        "            if child.attrib['Label'] == 'FINDINGS':\n",
        "                findings.append(child.text)\n",
        "        if child.tag == 'parentImage':\n",
        "            imageId.append(IMAGES_DIR+child.attrib['id']+'.png')\n",
        "    images.append(imageId)"
      ],
      "metadata": {
        "id": "Hg2QkAR9Oq5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(\n",
        "    {'ids': ids,\n",
        "     'images': images,\n",
        "     'impressions': impressions,\n",
        "     'findings': findings\n",
        "    })"
      ],
      "metadata": {
        "id": "VLK2clnhOs14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_count_0 = df.images.apply(lambda x:len(x)==0)\n",
        "idx = []\n",
        "for i in img_count_0.index.values:\n",
        "    if img_count_0[i]==True:\n",
        "        idx.append(i)\n",
        "df.drop(idx, inplace=True)\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "df['images'] = df['images'].apply(lambda x:x[:2])\n",
        "for i in range(len(df['images'])):\n",
        "  if len(df['images'][i]) < 2:\n",
        "    df['images'][i] = [df['images'][i][0], df['images'][i][0]]\n",
        "\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "JpoBzS8smofY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decontracted(phrase):\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def preprocess(text):\n",
        "  text = re.sub('XXXX','',text)\n",
        "  text = re.sub('xxxx','',text)\n",
        "  text = re.sub(r'[^A-Za-z.]+',' ',text) # removing periods too because impressions are one line\n",
        "  text = decontracted(text)\n",
        "  text = re.sub(r'[\\r\\n]', ' ', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "WLu2_ENimock"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['findings'] = df['findings'].map(preprocess)"
      ],
      "metadata": {
        "id": "u6JBfADdmoaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "4JO3XXImP0oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_images_split = df['images'].apply(pd.Series)\n",
        "df = pd.concat([df, df_images_split], axis=1)\n",
        "df.head()\n",
        "df.columns = ['index','uid', 'images', 'impressions', 'findings', 'image1', 'image2']\n",
        "df = df.drop(columns=['index', 'images'])\n",
        "# df['image2'].fillna(df['image1'], inplace=True)"
      ],
      "metadata": {
        "id": "P6DP_mtDnC9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "2R8QrK6ROSNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224"
      ],
      "metadata": {
        "id": "WOzz7yp6mnFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained VGG16 model\n",
        "# vgg16 = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "# trying chexnet\n",
        "pretrained_model = tf.keras.applications.DenseNet121(weights='/content/drive/My Drive/Colab Notebooks/Data/IUXRay/CheXNet_weights.h5',\n",
        "                                                classes = 14,input_shape=(IMG_HEIGHT,IMG_WIDTH,3))"
      ],
      "metadata": {
        "id": "m7DKHKhelF0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "M8Zceu3knC0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "XjaePkx2mo0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input\n",
        "\n",
        "def extract_image_features(filepath):\n",
        "    img = cv2.imread(filepath)\n",
        "    # print(filepath)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    img = tf.keras.applications.densenet.preprocess_input(img)\n",
        "    features = pretrained_model.predict(np.expand_dims(img, axis=0))\n",
        "    return features\n",
        "\n",
        "def extract_features(filepaths):\n",
        "    features = []\n",
        "    for filepath in filepaths:\n",
        "        features.append(extract_image_features(filepath))\n",
        "        if  len(features) %100 == 0:\n",
        "          print(len(features))\n",
        "    return np.array(features)"
      ],
      "metadata": {
        "id": "QEDFD5pplFqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting text features using TF-IDF\n",
        "def extract_text_features(text):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    # Fit vectorizer to the text data and transform into features\n",
        "    features = vectorizer.fit_transform(text)\n",
        "    return features.toarray()"
      ],
      "metadata": {
        "id": "0XQnAzShlFn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_features = extract_text_features(df['findings'])"
      ],
      "metadata": {
        "id": "YWPA4bzAyMmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image1_features = extract_features(df['image1'].values)\n"
      ],
      "metadata": {
        "id": "63rvsc5vyP9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "kDSXjucg0t4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image1_features_train = extract_features(train_df['image1'].values)\n",
        "image_features_train = [f.flatten() for f in image_features_train]\n",
        "text_features_train = extract_text_features(train_df['findings'])"
      ],
      "metadata": {
        "id": "ynLsQuO3lFiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features_train.shape, text_features_train.shape"
      ],
      "metadata": {
        "id": "q0-QlxGGlFcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_features_train[0])"
      ],
      "metadata": {
        "id": "57YXzk0KXE9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_train = [np.concatenate((img_feat, text_feat)) for img_feat, text_feat in zip(image_features_train, text_features_train)]\n",
        "num_dimensions = features_train[0].shape[0]"
      ],
      "metadata": {
        "id": "nYrB4nH_n5D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = AnnoyIndex(num_dimensions, metric=\"angular\")\n",
        "# adding feature vectors to the index\n",
        "for i, feature in enumerate(features_train):\n",
        "    index.add_item(i, feature)\n",
        "index.build(10)"
      ],
      "metadata": {
        "id": "TxPbWmQ6n5Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features_test = extract_features(test_df['image1'].values)\n",
        "# text_features_test = extract_text_features(test_df['findings'])"
      ],
      "metadata": {
        "id": "VJyQ2wo4n4-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 10\n",
        "true_findings = []\n",
        "predicted_findings = []\n",
        "\n",
        "for i, row in test_df.iterrows():\n",
        "    query_image_features = extract_image_features(row['image1'])\n",
        "    query_features = np.concatenate((query_image_features,), axis=None)\n",
        "    query_features = np.pad(query_features, (0, num_dimensions - len(query_features)), mode='constant')\n",
        "    nn_indices = index.get_nns_by_vector(query_features, k)\n",
        "    nn_findings = [train_df.iloc[i]['findings'] for i in nn_indices]\n",
        "    predicted_finding = max(nn_findings, key = nn_findings.count)\n",
        "    predicted_findings.append(predicted_finding)\n",
        "    true_findings.append(row['findings'])"
      ],
      "metadata": {
        "id": "1BGhJi6Cn47R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(predicted_findings), len(true_findings)"
      ],
      "metadata": {
        "id": "lvCW3TD3n44i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Ground Truth Report for a sample: {predicted_findings[0]}')\n",
        "print(f'Predicted Report for a sample: {true_findings[0]}')"
      ],
      "metadata": {
        "id": "N7Xn5QQyn41l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Ground Truth Report for a sample: {predicted_findings[40]}')\n",
        "print(f'Predicted Report for a sample: {true_findings[40]}')"
      ],
      "metadata": {
        "id": "r_ZgAn8uoGkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def compute_bleu_scores(predicted_findings, true_findings):\n",
        "    references = [[true.split()] for true in true_findings]\n",
        "    hypotheses = [predicted.split() for predicted in predicted_findings]\n",
        "\n",
        "    bleu_1 = corpus_bleu(references, hypotheses, weights=(1, 0, 0, 0))\n",
        "    bleu_2 = corpus_bleu(references, hypotheses, weights=(0.5, 0.5, 0, 0))\n",
        "    bleu_3 = corpus_bleu(references, hypotheses, weights=(1/3, 1/3, 1/3, 0))\n",
        "    bleu_4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    return bleu_1, bleu_2, bleu_3, bleu_4\n",
        "\n",
        "\n",
        "bleu_1, bleu_2, bleu_3, bleu_4 = compute_bleu_scores(predicted_findings, true_findings)\n",
        "\n",
        "print(f\"BLEU-1: {bleu_1}\")\n",
        "print(f\"BLEU-2: {bleu_2}\")\n",
        "print(f\"BLEU-3: {bleu_3}\")\n",
        "print(f\"BLEU-4: {bleu_4}\")"
      ],
      "metadata": {
        "id": "gh0fceACZ5k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "total_score = 0.0\n",
        "scores = []\n",
        "num_scores = 0\n",
        "for pred, true in zip(predicted_findings, true_findings):\n",
        "    pred_tokens = pred.split()\n",
        "    true_tokens = true.split()\n",
        "    score = sentence_bleu([true_tokens], pred_tokens)\n",
        "    scores.append(score)\n",
        "    total_score += score\n",
        "    num_scores += 1\n",
        "avg_score = total_score / num_scores\n",
        "\n",
        "print(f\"Average BLEU score: {avg_score}\")"
      ],
      "metadata": {
        "id": "6nofCHp2oGgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(scores, density=True, bins=30)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dA_GTD8zoGdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zkk3QGqoGaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4J9kpzfoGXI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}